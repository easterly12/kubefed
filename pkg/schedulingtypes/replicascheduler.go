/*
Copyright 2018 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package schedulingtypes

import (
	"bytes"
	"context"
	"fmt"
	"sort"
	"time"

	"github.com/pkg/errors"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	pkgruntime "k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/util/runtime"
	"k8s.io/klog"
	crclient "sigs.k8s.io/controller-runtime/pkg/client"

	"sigs.k8s.io/kubefed/pkg/apis/core/typeconfig"
	fedschedulingv1a1 "sigs.k8s.io/kubefed/pkg/apis/scheduling/v1alpha1"
	genericclient "sigs.k8s.io/kubefed/pkg/client/generic"
	ctlutil "sigs.k8s.io/kubefed/pkg/controller/util"
	"sigs.k8s.io/kubefed/pkg/controller/util/planner"
	"sigs.k8s.io/kubefed/pkg/controller/util/podanalyzer"
)

const (
	RSPKind = "ReplicaSchedulingPreference"
)

func init() {
	schedulingType := SchedulingType{
		Kind:             RSPKind,
		SchedulerFactory: NewReplicaScheduler,
	}
	RegisterSchedulingType("deployments.apps", schedulingType)
	RegisterSchedulingType("replicasets.apps", schedulingType)
}

type ReplicaScheduler struct {
	controllerConfig *ctlutil.ControllerConfig

	eventHandlers SchedulerEventHandlers

	plugins *ctlutil.SafeMap

	client      genericclient.Client
	podInformer ctlutil.FederatedInformer
}

func NewReplicaScheduler(controllerConfig *ctlutil.ControllerConfig, eventHandlers SchedulerEventHandlers) (Scheduler, error) {
	client := genericclient.NewForConfigOrDieWithUserAgent(controllerConfig.KubeConfig, "replica-scheduler")
	scheduler := &ReplicaScheduler{
		plugins:          ctlutil.NewSafeMap(),
		controllerConfig: controllerConfig,
		eventHandlers:    eventHandlers,
		client:           client,
	}

	// TODO: Update this to use a typed client from single target informer.
	// As of now we have a separate informer for pods, whereas all we need
	// is a typed client.
	// We ignore the pod events in this informer from clusters.
	var err error
	scheduler.podInformer, err = ctlutil.NewFederatedInformer(
		controllerConfig,
		client,
		PodResource,
		func(pkgruntime.Object) {},
		eventHandlers.ClusterLifecycleHandlers,
	)
	if err != nil {
		return nil, err
	}

	return scheduler, nil
}

func (s *ReplicaScheduler) SchedulingKind() string {
	return RSPKind
}

func (s *ReplicaScheduler) StartPlugin(typeConfig typeconfig.Interface) error {
	kind := typeConfig.GetFederatedType().Kind
	// TODO(marun) Return an error if the kind is not supported

	plugin, err := NewPlugin(s.controllerConfig, s.eventHandlers, typeConfig)
	if err != nil {
		return errors.Wrapf(err, "Failed to initialize replica scheduling plugin for %q", kind)
	}

	plugin.Start()
	s.plugins.Store(kind, plugin)

	return nil
}

func (s *ReplicaScheduler) StopPlugin(kind string) {
	plugin, ok := s.plugins.Get(kind)
	if !ok {
		return
	}

	plugin.(*Plugin).Stop()
	s.plugins.Delete(kind)
}

func (s *ReplicaScheduler) ObjectType() pkgruntime.Object {
	return &fedschedulingv1a1.ReplicaSchedulingPreference{}
}

func (s *ReplicaScheduler) Start() {
	s.podInformer.Start()
}

func (s *ReplicaScheduler) HasSynced() bool {
	for _, plugin := range s.plugins.GetAll() {
		if !plugin.(*Plugin).HasSynced() {
			return false
		}
	}

	if !s.podInformer.ClustersSynced() {
		klog.V(2).Infof("Cluster list not synced")
		return false
	}
	clusters, err := s.podInformer.GetReadyClusters()
	if err != nil {
		runtime.HandleError(errors.Wrap(err, "Failed to get ready clusters"))
		return false
	}
	return s.podInformer.GetTargetStore().ClustersSynced(clusters)
}

func (s *ReplicaScheduler) Stop() {
	for _, plugin := range s.plugins.GetAll() {
		plugin.(*Plugin).Stop()
	}
	s.plugins.DeleteAll()
	s.podInformer.Stop()
}

// Read-Note: ç›®å‰å¯¹äºŽ Scheduler æœ€é‡ç‚¹çš„å…¶å®žå°±åœ¨è¿™äº†ï¼Œå¯¹äºŽ Replicas å­—æ®µçš„è°ƒå
// è€Œè¿™æ®µå…¶å®žå°±æ˜¯æžå·²ç»æ³¨å†Œçš„ Type çš„ Plugin ï¼Œæ‰§è¡Œä»–çš„è°ƒåï¼Œæ‰€ä»¥å†å¾€ä¸‹æ‰Žä¸€å±‚ï¼Œçœ‹ Plugin çš„ Reconcile
func (s *ReplicaScheduler) Reconcile(obj pkgruntime.Object, qualifiedName ctlutil.QualifiedName) ctlutil.ReconciliationStatus {
	rsp, ok := obj.(*fedschedulingv1a1.ReplicaSchedulingPreference)
	if !ok {
		runtime.HandleError(errors.Errorf("Incorrect runtime object for RSP: %v", rsp))
		return ctlutil.StatusError
	}

	clusterNames, err := s.clusterNames()
	if err != nil {
		runtime.HandleError(errors.Wrap(err, "Failed to get cluster list"))
		return ctlutil.StatusError
	}
	if len(clusterNames) == 0 {
		// no joined clusters, nothing to do
		return ctlutil.StatusAllOK
	}

	kind := rsp.Spec.TargetKind
	if kind != "FederatedDeployment" && kind != "FederatedReplicaSet" {
		runtime.HandleError(errors.Wrapf(err, "RSP target kind: %s is incorrect", kind))
		return ctlutil.StatusNeedsRecheck
	}

	plugin, ok := s.plugins.Get(kind)
	if !ok {
		return ctlutil.StatusAllOK
	}

	if !plugin.(*Plugin).FederatedTypeExists(qualifiedName.String()) {
		// target FederatedType does not exist, nothing to do
		return ctlutil.StatusAllOK
	}

	key := qualifiedName.String()
	// Read-Note: å…³é”®åœ¨äºŽè¿™ä¸ª result ï¼Œä¹Ÿå°±æ˜¯ replicas map çš„èŽ·å–
	result, err := s.GetSchedulingResult(rsp, qualifiedName, clusterNames)
	if err != nil {
		runtime.HandleError(errors.Wrapf(err, "Failed to compute the schedule information while reconciling RSP named %q", key))
		return ctlutil.StatusError
	}

	// Read-Note: å…¶å®žå°±æ˜¯æŠŠ placement + override + replicas schedule çš„å†…å®¹å®žé™…è¿›è¡Œç»„åˆï¼Œè¿›è¡Œå¿…è¦çš„ update
	err = plugin.(*Plugin).Reconcile(qualifiedName, result)
	if err != nil {
		runtime.HandleError(errors.Wrapf(err, "Failed to reconcile federated targets for RSP named %q", key))
		return ctlutil.StatusError
	}

	return ctlutil.StatusAllOK
}

// The list of clusters could come from any target informer
func (s *ReplicaScheduler) clusterNames() ([]string, error) {
	clusters, err := s.podInformer.GetReadyClusters()
	if err != nil {
		return nil, err
	}
	clusterNames := []string{}
	for _, cluster := range clusters {
		clusterNames = append(clusterNames, cluster.Name)
	}

	return clusterNames, nil
}

// Read-Note: å…³é”®ä¸­çš„å…³é”®ï¼Œreplicas çš„æ•°é‡æ˜¯å¦‚ä½•é‡æ–°åˆ†é…çš„
func (s *ReplicaScheduler) GetSchedulingResult(rsp *fedschedulingv1a1.ReplicaSchedulingPreference, qualifiedName ctlutil.QualifiedName, clusterNames []string) (map[string]int64, error) {
	key := qualifiedName.String()

	// Read-Note: é€šè¿‡ key ä»Ž Cache èŽ·å– FT æŒ‡å‘çš„ç›®æ ‡ Type çš„ Resourceï¼Œè¿™é‡Œä¹Ÿå°±æ˜¯æŸä¸€é›†ç¾¤çš„ Deployment æˆ–è€… RS
	objectGetter := func(clusterName, key string) (interface{}, bool, error) {
		plugin, ok := s.plugins.Get(rsp.Spec.TargetKind)
		if !ok {
			return nil, false, nil
		}
		return plugin.(*Plugin).targetInformer.GetTargetStore().GetByKey(clusterName, key)
	}
	// Read-Note: é€šè¿‡ FT æŒ‡å‘çš„ç›®æ ‡ Type çš„ Resourceï¼ˆDeployment æˆ–è€… RSï¼‰ï¼ŒèŽ·å– spec.selector.matchLabels
	// èŽ·å–åœ¨æŸä¸€é›†ç¾¤é‡Œç”±è¯¥ Resource åš Controller çš„ Pod åˆ—è¡¨
	podsGetter := func(clusterName string, unstructuredObj *unstructured.Unstructured) (*corev1.PodList, error) {
		client, err := s.podInformer.GetClientForCluster(clusterName)
		if err != nil {
			return nil, err
		}
		selectorLabels, ok, err := unstructured.NestedStringMap(unstructuredObj.Object, "spec", "selector", "matchLabels")
		if !ok {
			return nil, errors.New("missing selector on object")
		}
		if err != nil {
			return nil, errors.Wrap(err, "error retrieving selector from object")
		}

		podList := &corev1.PodList{}
		err = client.List(context.Background(), podList, unstructuredObj.GetNamespace(), crclient.MatchingLabels(selectorLabels))
		if err != nil {
			return nil, err
		}
		return podList, nil
	}

	// Read-Note: ç»¼åˆè¿™äº›å› ç´ é‡æ–°è€ƒè™‘å¦‚ä½•é‡æ–°æŽ’å¸ƒå„ä¸ªé›†ç¾¤çš„ replicas å æ¯”å’Œè¯„ä¼°åŽçš„èµ„æºä½¿ç”¨é‡ï¼ˆï¼Ÿï¼‰
	currentReplicasPerCluster, estimatedCapacity, err := clustersReplicaState(clusterNames, key, objectGetter, podsGetter)
	if err != nil {
		return nil, err
	}

	// Read-Note: çœ‹åˆ°è¿™ä¸ªç†Ÿæ‚‰çš„æ¯”ä¾‹åˆ†é…ç»“æž„ï¼Œå¯¹äºŽæ²¡æœ‰è®¾å®šé›†ç¾¤çš„æƒ…å†µï¼Œåˆ™è®¤ä¸ºæ‰€æœ‰é›†ç¾¤åŒæƒå¹³åˆ†
	// éœ€è¦åæ§½è¿™ preference çŽ©æ„å±…ç„¶è¿˜å¥—äº† min max ï¼Œæ•´ä½“é€»è¾‘å¤æ‚åº¦ðŸ’¥ä¸Šå¤©
	// TODO: Move this to API defaulting logic
	if len(rsp.Spec.Clusters) == 0 {
		rsp.Spec.Clusters = map[string]fedschedulingv1a1.ClusterPreferences{
			"*": {Weight: 1},
		}
	}

	plnr := planner.NewPlanner(rsp)
	return schedule(plnr, key, clusterNames, currentReplicasPerCluster, estimatedCapacity)
}

func schedule(planner *planner.Planner, key string, clusterNames []string, currentReplicasPerCluster map[string]int64, estimatedCapacity map[string]int64) (map[string]int64, error) {
	scheduleResult, overflow, err := planner.Plan(clusterNames, currentReplicasPerCluster, estimatedCapacity, key)
	if err != nil {
		return nil, err
	}

	// TODO: Check if we really need to place the federated type in clusters
	// with 0 replicas. Override replicas would be set to 0 in this case.
	result := make(map[string]int64)
	for clusterName := range currentReplicasPerCluster {
		result[clusterName] = 0
	}

	for clusterName, replicas := range scheduleResult {
		result[clusterName] = replicas
	}
	for clusterName, replicas := range overflow {
		result[clusterName] += replicas
	}

	if klog.V(4) {
		buf := bytes.NewBufferString(fmt.Sprintf("Schedule - %q\n", key))
		sort.Strings(clusterNames)
		for _, clusterName := range clusterNames {
			cur := currentReplicasPerCluster[clusterName]
			target := scheduleResult[clusterName]
			fmt.Fprintf(buf, "%s: current: %d target: %d", clusterName, cur, target)
			if over, found := overflow[clusterName]; found {
				fmt.Fprintf(buf, " overflow: %d", over)
			}
			if capacity, found := estimatedCapacity[clusterName]; found {
				fmt.Fprintf(buf, " capacity: %d", capacity)
			}
			fmt.Fprintf(buf, "\n")
		}
		klog.V(4).Infof(buf.String())
	}
	return result, nil
}

// clustersReplicaState returns information about the scheduling state of the pods running in the federated clusters.
func clustersReplicaState(
	clusterNames []string,
	key string,
	objectGetter func(clusterName string, key string) (interface{}, bool, error),
	podsGetter func(clusterName string, obj *unstructured.Unstructured) (*corev1.PodList, error)) (currentReplicasPerCluster map[string]int64, estimatedCapacity map[string]int64, err error) {
	currentReplicasPerCluster = make(map[string]int64)
	estimatedCapacity = make(map[string]int64)

	for _, clusterName := range clusterNames {
		// Read-Note: èŽ·å– Deployment æˆ– RS çš„ spec.replicas å’Œ status.readyReplicas
		obj, exists, err := objectGetter(clusterName, key)
		if err != nil {
			return nil, nil, err
		}
		if !exists {
			continue
		}

		unstructuredObj := obj.(*unstructured.Unstructured)
		// Read-Questionï¼šå¦‚æžœæ˜¯æ‰¾ä¸åˆ° spec.replicas å­—æ®µçš„æƒ…å†µå°±ç›´æŽ¥è®¤ä¸ºæ˜¯ 0 æœ‰ç‚¹ä¸å¤ªåˆç†
		// å› ä¸º RS å’Œ Deployment Spec çš„ replicas çš„ç¼ºçœå«ä¹‰æ˜¯ä»¥ 1 ä½œä¸ºé»˜è®¤å€¼
		// è™½ç„¶ç†æƒ³çš„çŠ¶æ€æ˜¯è¯¥å­—æ®µæœ‰å€¼ï¼Œä½†æ˜¯ç¼ºçœçš„å­—æ®µå’ŒåŽŸå®šä¹‰ä¸ªäººè®¤ä¸ºæœ€å¥½ä¿æŒä¸€è‡´
		replicas, ok, err := unstructured.NestedInt64(unstructuredObj.Object, "spec", "replicas")
		if err != nil {
			return nil, nil, errors.Wrap(err, "Error retrieving 'replicas' field")
		}
		if !ok {
			replicas = int64(0)
		}
		// Read-Question: å¦‚æžœæ˜¯æ‰¾ä¸åˆ° status.readyReplicas ä¹Ÿç›´æŽ¥è®¤ä¸ºæ˜¯ 0 ä¹Ÿä¸å¤ªåˆç†
		// è¿™ä¸ªå­—æ®µæœ¬èº«åœ¨ RS å’Œ Deployment ä¸ºéž ptr å­—æ®µï¼Œå¦‚æžœ not found å¾ˆå¤§ç¨‹åº¦ä¸Šå¯ä»¥æ€€ç–‘æ˜¯ä¸Šå±‚ä¼ å…¥çš„ unstructed object ç±»åž‹ä¸ç¬¦åˆé¢„æœŸ
		// æ›´åˆç†çš„åšæ³•åº”è¯¥æ˜¯ä¹Ÿè¿”å›ž error ä¸­æ­¢è¿™æ¬¡ replicas é‡åˆ†é…çš„è®¡ç®—
		readyReplicas, ok, err := unstructured.NestedInt64(unstructuredObj.Object, "status", "readyreplicas")
		if err != nil {
			return nil, nil, errors.Wrap(err, "Error retrieving 'readyreplicas' field")
		}
		if !ok {
			readyReplicas = int64(0)
		}

		if replicas == readyReplicas {
			// Read-Note: å¦‚æžœå½“å‰é›†ç¾¤çš„å®žä¾‹æ•°å³ä¸ºæœŸæœ›çŠ¶æ€ï¼Œé‚£ç»´æŒçŽ°æœ‰çš„ replicas å°±è¡Œ
			currentReplicasPerCluster[clusterName] = readyReplicas
		} else {
			// Read-Note: å¦‚æžœå½“å‰é›†ç¾¤çš„å®žä¾‹æ•°ä¸ŽæœŸæœ›çŠ¶æ€æœ‰å‡ºå…¥ï¼Œé‚£ä¹ˆå°±éœ€è¦ç»“åˆè¯¥ Controller ç®¡ç†çš„æ‰€æœ‰ Pod çš„çŠ¶å†µæ¥è€ƒè™‘
			// Read-Question: çœ‹åˆ°è¿™é‡Œä¼šæœ‰å’Œä¸Šæ–‡æœ‰ç±»ä¼¼çš„ç–‘é—®ï¼Œè¿™é‡Œç»™ current replicas per cluster è®¾ç½®åˆå§‹å€¼ä¸º 0 æ˜¯å¦åˆç†ï¼ˆå½“ç„¶è¿™ä¸ªåˆå§‹åŒ– 0 æœ€ç»ˆè‚¯å®šä¸ä¼šé‡‡ç”¨ï¼‰
			// Read-Questionï¼šè¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯è¿™è¾¹é¢å¤–ç»Ÿè®¡ podStatus.RunningAndReady å¯ä»¥è®¤ä¸ºå’Œå‰æ–‡çš„ readyReplicas ç­‰ä»·ï¼Œè¿™è¾¹å¤šåšä¸€æ¬¡ç»Ÿè®¡æ˜¯è€ƒè™‘åˆ° Status çŠ¶æ€å¯èƒ½æ²¡æœ‰åŠæ—¶æ›´æ–°ï¼Ÿ
			// Read-Question: é™¤æ­¤ä¹‹å¤–è¿˜æƒ³åˆ°äº†ä¸€ä¸ªåœºæ™¯ï¼Œå¦‚æžœæ˜¯æ­£åœ¨æ»šåŠ¨å‡çº§çš„çŠ¶æ€ï¼Œrunning&ready > spec.replicas ï¼Œè¿™è¾¹ç›´æŽ¥ä¸¢è¿›åŽ»ä¼¼ä¹Žé—®é¢˜â€¦å¾ˆå¤§å‘€â€¦â€¦
			// è¿™é‡Œå…ˆç”»ä¸ªæœªå®Œå¾…ç»­ï¼Œçœ‹çœ‹å¤–å±‚å…·ä½“æ€Žä¹ˆç”¨è¿™é‡Œçš„è®¡ç®—ç»“æžœï¼Œå…·ä½“åœ¨ `pkg/controller/util/planner/planner.go` ä¸­ `planner.Plan` æ˜¯æ€Žä¹ˆå®žçŽ°çš„
			currentReplicasPerCluster[clusterName] = int64(0)
			podList, err := podsGetter(clusterName, unstructuredObj)
			if err != nil {
				return nil, nil, err
			}

			podStatus := podanalyzer.AnalyzePods(podList, time.Now())
			currentReplicasPerCluster[clusterName] = int64(podStatus.RunningAndReady) // include pending as well?
			// Read-Question: è¿™è¾¹ç»Ÿè®¡çš„ unschedule æ˜¯ä»¥ condition èŽ·å–åˆ° pod è¢«è°ƒåº¦å™¨ 1min å†…å°è¯•è°ƒåº¦ï¼Œä½†æ˜¯æœ€ç»ˆæ— æ³•è°ƒåº¦æˆåŠŸ
			// æ­¤å¤„è€ƒè™‘åˆ°å¯èƒ½æ˜¯é›†ç¾¤èµ„æºåŽŸå› ã€Quota é™åˆ¶ã€è°ƒåº¦æ¡ä»¶ï¼ˆNode Selectorã€äº²å’Œå’Œåäº²å’Œï¼‰
			// è€Œè¿™äº›çŠ¶å†µéƒ½æ˜¯ä¸€ä¸ªåŠ¨æ€è¿‡ç¨‹ï¼Œè¿™ç§æƒ…å†µå¦‚æžœè€ƒè™‘ç›´æŽ¥æŠŠè¿™ä»½å®žä¾‹é‡ç»™åˆ°å…¶ä»–é›†ç¾¤ï¼Œå¹¶ä¸æ˜¯é’ˆå¯¹æ¯ä¸€ç§æƒ…å†µéƒ½åˆç†ï¼Œ
			// æ¯”å¦‚ CA å¯å¿«é€Ÿæ‰©å®¹ï¼Œå…¶ä»–æœ‰ä¸€äº›ä¸åˆç†çš„è°ƒåº¦æ¡ä»¶è¢«ç§»é™¤ï¼Œè€Œä¸”å°±ç®—è°ƒåº¦åˆ°å…¶ä»–é›†ç¾¤ä¹Ÿæœ‰å¯èƒ½å¼•å‘æ–°çš„ Unschedulable ç„¶åŽæ¥å›žæ”¾
			// Read-Question: è¿™è¾¹ä¹Ÿæ²¡æœ‰è€ƒè™‘åˆ° pod æ˜¯å¦æ˜¯ updated ï¼Œæ‰€ä»¥æ–°æ—§ pod æ˜¯åŒæ—¶è€ƒè™‘çš„ï¼Œå¯èƒ½åªè€ƒè™‘ updated çš„éƒ¨åˆ†ä¼šæ›´åˆç†ä¸€äº›
			unschedulable := int64(podStatus.Unschedulable)
			if unschedulable > 0 {
				estimatedCapacity[clusterName] = replicas - unschedulable
			}
		}
	}
	return currentReplicasPerCluster, estimatedCapacity, nil
}
